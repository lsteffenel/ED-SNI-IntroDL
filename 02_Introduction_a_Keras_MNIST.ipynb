{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"TP1_Some_basics_about_learning_process.ipynb","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Construire un modèle de réseaux de neurones avec Keras\n\nKeras est une bibliothèque \"haut niveau\" utilisée pour simplifier la description de modèles de réseaux de neurones sur Tensorflow (bibliothèque IA de Google). L'avantage surtout est de pouvoir utiliser des GPU pour accélérer le calcul.\n\nLe travail avec Keras suit un cheminement similaires à celui avec Scikit-Learn, mais il y a quelques différences à retenir.","metadata":{"id":"EpQRZSswWmWI"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nprint (tf.__version__)\nkeras.__version__","metadata":{"id":"nO3TA6f7WmWJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dans le paragraphe suivant vous avez certainement eu un message d'erreur indiquant que vous n'avez pas des GPU. Dans ce cas, Keras utilisera la CPU de la machine.\n\n## Chargement de données\n\nTout comme Scikit-Learn, Keras a aussi un ensemble de datasets prêt à utilisation pour des exemples. Dans le cas suivant, nous allons charger le dataset MNIST (écriture à la main) et le séparer en deux groupes : Train et Test. Les données de validation (vérification pendant l'entraînement) seront séparés du groupe Train plus tard.","metadata":{"id":"5eYo87D2WmWJ"}},{"cell_type":"code","source":"from keras.datasets import mnist\n\n#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","metadata":{"id":"q2eLH6ACWmWJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape\nx_test.shape\ny_test.shape","metadata":{"id":"odgDUCJSWmWJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"id":"iOXq4OkDWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les données de MNIST se présentent sous la forme d'images 28x28 pixels, avec 256 tons de gris. Les labels (`y_train`, par exemple) correspondent aux caractères représentés : les chiffres 0 à 9.\n\nLe paragraphe suivant définit une fonction permettant de visualiser ce dataset.","metadata":{"id":"8cNaXhPsWmWK"}},{"cell_type":"code","source":"def plot_images(x,y=None, indices='all', columns=12, x_size=1, y_size=1,\n                colorbar=False, y_pred=None, cm='binary', norm=None, y_padding=0.35, spines_alpha=1,\n                fontsize=20, interpolation='lanczos'):\n \n    if indices=='all': indices=range(len(x))\n    if norm and len(norm) == 2: norm = matplotlib.colors.Normalize(vmin=norm[0], vmax=norm[1])\n    draw_labels = (y is not None)\n    draw_pred   = (y_pred is not None)\n    rows        = math.ceil(len(indices)/columns)\n    fig=plt.figure(figsize=(columns*x_size, rows*(y_size+y_padding)))\n    n=1\n    for i in indices:\n        axs=fig.add_subplot(rows, columns, n)\n        n+=1\n        xx=x[i]\n        # ---- Shape is (lx,ly)\n        if len(x[i].shape)==2:\n            xx=x[i]\n        # ---- Shape is (lx,ly,n)\n        if len(x[i].shape)==3:\n            (lx,ly,lz)=x[i].shape\n            if lz==1: \n                xx=x[i].reshape(lx,ly)\n            else:\n                xx=x[i]\n        img=axs.imshow(xx,   cmap = cm, norm=norm, interpolation=interpolation)\n#         img=axs.imshow(xx,   cmap = cm, interpolation=interpolation)\n        axs.spines['right'].set_visible(True)\n        axs.spines['left'].set_visible(True)\n        axs.spines['top'].set_visible(True)\n        axs.spines['bottom'].set_visible(True)\n        axs.spines['right'].set_alpha(spines_alpha)\n        axs.spines['left'].set_alpha(spines_alpha)\n        axs.spines['top'].set_alpha(spines_alpha)\n        axs.spines['bottom'].set_alpha(spines_alpha)\n        axs.set_yticks([])\n        axs.set_xticks([])\n        if draw_labels and not draw_pred:\n            axs.set_xlabel(y[i],fontsize=fontsize)\n        if draw_labels and draw_pred:\n            axs.set_xlabel(y[i],fontsize=fontsize)\n        if colorbar:\n            fig.colorbar(img,orientation=\"vertical\", shrink=0.65)\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons donc afficher l'un des caractères du groupe Train (celui à la position 27) mais aussi tout la plage entre les données 5 et 41. Remarquez le \"label\" correspondant sous chaque image.","metadata":{}},{"cell_type":"code","source":"plot_images(x_train, y_train, [27],  x_size=5,y_size=5, colorbar=True)\nplot_images(x_train, y_train, range(5,41), columns=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les paragraphes suivants font plusieurs opérations afin de préparer les données :\n\n**1 - Reformater les données**\n\nLes images 28x28 sont \"applaties\" en un seul array unidimensionnel de 784 valeurs","metadata":{"id":"8cNaXhPsWmWK"}},{"cell_type":"code","source":"x_train = x_train.reshape(60000, 784) #  28*28\nx_test = x_test.reshape(10000, 784)","metadata":{"id":"vJ-_e5bQWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2 - Transformation et normalisation des données**\n\nLes valeurs de base son des entiers entre 0 et 255 pour représenter les 256 tons de gris. La majorité des algorithmes utilisent des valeurs réels, de préférence dans la fourchette 0 à 1 ou -1 à 1. \n\nLes paragraphes suivantes modifient le type des données (`float32`) puis font une normalisation simple (diviser la valeur par 255). Bien sûr, d'autres méthodes de normalisation plus élaborées sont possibles, mais ça suffit pour l'instant.","metadata":{}},{"cell_type":"code","source":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')","metadata":{"id":"vJ-_e5bQWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train /= 255\nx_test /= 255","metadata":{"id":"vJ-_e5bQWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3 - Transformer les données catégoriques**\nLorsqu'on a des données catégoriques (texte ou numéros), il faut les transformer afin d'éviter des mauvaises compréhensions de la part de l'algorithme (par exemple, supposer que une classe 2 vient toujours après une classe 1). Dans notre cas, nous allons transformer les classes 0 à 9 en représentations numériques (similaire à HotOneEncoder de Sklearn), afin de rendre indépendantes ces classes.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ny_train = keras.utils.to_categorical(y_train, num_classes=10)\ny_test = keras.utils.to_categorical(y_test, num_classes=10)","metadata":{"id":"vJ-_e5bQWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"id":"5RGjh84XWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## La Création d'un modèle\n\nKeras a plusieurs modes permettant la création de modèles de réseaux de neurones. Dans ce cas, nous allons utiliser l'API `Sequential` qui permet de décrire couche par couche du réseau et les empiler (grâce à `add()`). \n\nNous allons faire un modèle simple avec des réseaux denses (totalement connectés). La première couche définit la taille de l'entrée (les 784 valeurs reçus du dataset), les autres utilisent par défaut la taille de la sortie de la couche précédente. Egalement, nous indiquons que chaque couche comptera avec 10 neurones.\n\nFinalemen, remarquez qu'on utilise deux types de fonction d'activation, sigmoid et softmax.\nPour simplifier la description, sigmoid donne une probabilité entre 0 et 1, alors que Softmax affiche \"1\" sur la sortie avec la plus grande probabilité et \"0\" sur les autres. C'est pour cela qu'on utilise Softmax à la sortie, ça permet d'avoir un résultat plutôt qu'une liste de probabilités.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n\n# Declaration du modèle en Tensorflow2.0\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid', input_dim =(784)))\nmodel.add(tf.keras.layers.Dense(10, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n\n# résumé du modèle\nmodel.summary()\n\n","metadata":{"id":"U9XLxq-BWmWK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Entraînement du modèle\n\nUne fois défini le modèle, il faut l'entraîner avec les données. \nLe paragraphe suivant définit les hyperparamètres du modèle, dont le `batch_size`(taille des sous-ensembles utilisés dans la descente de gradient), le nombre d'epochs (parcours de l'ensemble de données d'entraînement).\n\nL'appel à compile indique aussi qu'on utilise le modèle de descente de gradient SGD (il y a plusieurs), que la métrique utilisée est l'accuracy (métrique qui correspond à (TP+TN)/(TP+TN+FP+FN)), et que la fonction de perte est la `categorical_crossentropy`, une fonction qui compare les probabilités pour des labels catégoriques.","metadata":{}},{"cell_type":"code","source":"batch_size = 100\n#num_classes = 10\nepochs= 50\n\nmodel.compile(loss='categorical_crossentropy',  optimizer='SGD',  metrics=['accuracy'])\n","metadata":{"id":"32wR8ClWWmWL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalement, on lance l'entraînement. Remarquez aussi qu'on n'a pas crée des données Validation avant, on le fera ici en réservant 10% des données de Train. \n\nComme le dataset est simple, on peut faire 50 epoch même sans un GPU.","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,verbose=1 )\n\n#verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n# Je vous invite à lire la documentation : https://keras.io/models/sequential/","metadata":{"id":"Jng3P9z1WmWL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Les paragraphes suivants nous permettent de voir comment le modèle améliore sa performance au fil des epochs","metadata":{}},{"cell_type":"code","source":"def plot_history(history, figsize=(8,6), \n                 plot={\"Accuracy\":['accuracy','val_accuracy'], 'Loss':['loss', 'val_loss']}):\n    \"\"\"\n    Show history\n    args:\n        history: history\n        figsize: fig size\n        plot: list of data to plot : {<title>:[<metrics>,...], ...}\n    \"\"\"\n    fig_id=0\n    for title,curves in plot.items():\n        plt.figure(figsize=figsize)\n        plt.title(title)\n        plt.ylabel(title)\n        plt.xlabel('Epoch')\n        for c in curves:\n            plt.plot(history.history[c])\n        plt.legend(curves, loc='upper left')\n        \n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history, figsize=(6,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Enfin, on peut estimer la performance du modèle avec les données Test. \n\nComparez ces valeur avec ceux de l'entraînement (`val_loss` et `val_accuracy``\n ci-dessus).","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(x_test, y_test,verbose=0)\n\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)","metadata":{"id":"qTbNPwg6WmWL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ces résultats montrent que le modèle se porte un peu moins bien avec de nouvelles données, mais ça reste intéressant. ","metadata":{}},{"cell_type":"markdown","source":"## Exercice : \nOn a obtenu avec ce modèle basique, un taux d'accuracy d'environ 87%.\n- Essayer d'améliorer la performence du modèle, en modifiant les fonctions d'activation, ou/et en n ajoutant le nombre de neurones et des couches intermédiaires.\n\n","metadata":{"id":"0G4jaLW6WmWL"}},{"cell_type":"code","source":"","metadata":{"id":"u_TTk0YGWmWL","jupyter":{"source_hidden":true},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bonus : Quelques exemples de fonctions d'activation de base\n\n","metadata":{"id":"8_w-7ZF9WmWC"}},{"cell_type":"markdown","source":"### Activation functions","metadata":{"id":"Kd01IKzdWmWD"}},{"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nx = np.arange(-6, 6, 0.1)","metadata":{"id":"ShyJUe2VWmWE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear  :  Fonction d'activation linéaire\n\nC'est une fonction simple de la forme: f(x) = ax ou f(x) = x. En gros, l'entrée passe à la sortie sans une très grande modification ou alors sans aucune modification. ","metadata":{"id":"K_RTo7D7WmWE"}},{"cell_type":"code","source":"def linear(x):\n    a = []\n    for item in x:\n        a.append(item)\n    return a\n\ny = linear(x)\n\nplt.plot(x,y)\nplt.grid()\nplt.show()\n","metadata":{"id":"yoxBCvBSWmWF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sigmoid\n\nEn mathématiques, la fonction sigmoïde (dite aussi courbe en S) est définie par :\n\n$$ f(x)=\\frac{1}{1 + e^{- x}}$$ pour tout réel x ;\n\n\nLe but premier de la fonction est de réduire la valeur d'entrée pour la réduire entre 0 et 1. En plus d'exprimer la valeur sous forme de probabilité, si la valeur en entrée est un très grand nombre positif, la fonction convertira cette valeur en une probabilité de 1. A l'inverse, si la valeur en entrée est un très grand nombre négatif, la fonction convertira cette valeur en une probabilité de 0. D'autre part, l'équation de la courbe est telle que, seules les petites valeurs influent réellement sur la variation des valeurs en sortie.\n\nLa fonction Sigmoïde a plusieurs défaults:\n\n- Elle n'est pas centrée sur zéro, c'est à dire que des entrées négatives peuvent engendrer des sorties positives.\n\n- Etant assez plate, elle influe assez faiblement sur les neurones par rapport à d'autres fonctions d'activations. Le résultat est souvent très proche de 0 ou de 1 causant la saturation de certains neurones.\n\n- Elle est couteuse en terme de calcul car elle comprend la fonction exponentielle.\n","metadata":{"id":"hd_1He_kWmWG"}},{"cell_type":"code","source":"def sigmoid(x):\n    a = []\n    for item in x:\n        a.append(1/( (1+math.exp(-item) * 1))) \n        # ici j'ai pri A = 1 : plus A est grand plus on se rapproche à la fonction echelon ...\n        \n    return a\n\ny = sigmoid(x)\n\nplt.plot(x,y)\nplt.grid()\nplt.show()\n","metadata":{"scrolled":true,"id":"XiQyZuPMWmWG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tahn :  Tangente Hyperbolique \n\nCette fonction ressemble à la fonction Sigmoïde. La différence avec la fonction Sigmoïde est que la fonction Tanh produit un résultat compris entre -1 et 1. La fonction Tanh est en terme général préférable à la fonction Sigmoïde car elle est centrée sur zéro. Les grandes entrées négatives tendent vers -1 et les grandes entrées positives tendent vers 1.\n\nMis à part cet avantage, la fonction Tanh possède les mêmes autres inconvénients que la fonction Sigmoïde.","metadata":{"id":"N6pM0PdtWmWH"}},{"cell_type":"code","source":"def tanh(x, derivative=False):\n    if (derivative == True):\n        return (1 - (x ** 2))\n    return np.tanh(x)\n\n\ny = tanh(x)\n\nplt.plot(x,y)\nplt.grid()\nplt.show()","metadata":{"id":"lL-Po6KzWmWH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ReLU : Unité de Rectification Linéaire\nPour résoudre le problème de saturation des deux fonctions précédentes (Sigmoïde et Tanh) il existe la fonction ReLU (Unité de Rectification Linéaire). Cette fonction est la plus utilisée.\n\nLa fonction ReLU est inteprétée par la formule: f(x) = max(0, x). Si l'entrée est négative la sortie est 0 et si elle est positive, alors la sortie est égale à x. Cette fonction d'activation augmente considérablement la convergence du réseau et ne sature pas.\n\nMais la fonction ReLU n'est pas parfaite. Si la valeur d'entrée est négative, le neurone reste inactif, ainsi les poids ne sont pas mis à jour et le réseau n’apprend pas.","metadata":{"id":"Su5qkZlWWmWI"}},{"cell_type":"code","source":"def relu(x):\n    a = []\n    for item in x:\n        if item > 0:\n            a.append(item)\n        else:\n            a.append(0)\n    return a\n\n\ny = relu(x)\n\nplt.plot(x,y)\nplt.grid()\nplt.show()","metadata":{"id":"NaHM-WDRWmWI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}